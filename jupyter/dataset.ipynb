{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb848e9b43789d1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-04T01:51:28.893962Z",
     "start_time": "2024-03-04T01:51:25.218156Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "from typing import Tuple, List, Dict, Any\n",
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "import shared\n",
    "\n",
    "\n",
    "def convert_doc_to_text(\n",
    "        input_dir: str,\n",
    "        output_dir: str,\n",
    "        split_pattern: str | None = None,\n",
    "        min_length: int | None = None,\n",
    "        split_size: int = 10000,\n",
    "):\n",
    "    min_length = min_length or 0\n",
    "    for file in os.listdir(input_dir):\n",
    "        p = Path(input_dir, file)\n",
    "        if p.is_dir():\n",
    "            continue\n",
    "        match p.suffix.lower():\n",
    "            case \".txt\":\n",
    "                with p.open(mode=\"r\", encoding=\"utf8\") as f:\n",
    "                    text = f.read().replace(\"```\", \"\")\n",
    "            case \".docx\":\n",
    "                if p.name.startswith(\"~$\"):\n",
    "                    continue\n",
    "                text = data.doc.load_from_file(p)\n",
    "                text = \"\".join(text)\n",
    "            case \".pdf\":\n",
    "                text = data.pdf.load_from_file(p)\n",
    "            case _:\n",
    "                raise ValueError(f\"Unsupported file type {p.suffix}\")\n",
    "        if not text:\n",
    "            raise ValueError(\"未查找到内容\")\n",
    "        if split_pattern:\n",
    "            text = re.split(split_pattern, text)\n",
    "        if not isinstance(text, list):\n",
    "            if split_size:\n",
    "                text_ = []\n",
    "                for i in range(0, len(text), split_size):\n",
    "                    if len(text) - i >= split_size:\n",
    "                        text_.append(text[i:i+split_size])\n",
    "                    else:\n",
    "                        text_.append(text[i:])\n",
    "                text = text_\n",
    "            else:\n",
    "                text = [text, ]\n",
    "        for t in text:\n",
    "            if len(t) <= min_length:\n",
    "                continue\n",
    "            id_ = shared.snow.sid()\n",
    "            p = Path(output_dir, f\"{id_}.txt\")\n",
    "            with p.open(mode=\"w\", encoding=\"utf8\") as f:\n",
    "                f.write(t)\n",
    "\n",
    "\n",
    "def extract_qa_from_llm(\n",
    "        text: str,\n",
    "        token: str = \"gxllm\",\n",
    "        url: str = \"http://10.133.95.100:9100/v1\",\n",
    "        count: int = 15,\n",
    "):\n",
    "    if not text:\n",
    "        raise ValueError(\"context is invalid\")\n",
    "    if count:\n",
    "        count = f\"数量不少于{count}个\"\n",
    "    else:\n",
    "        count = \"提取尽可能多的问题答案\"\n",
    "    prompt = f\"\"\"\n",
    "将下述内容提炼问题和答案，要求{count}，答案尽可能详细，不得重复。\n",
    "格式要求如下：\n",
    "\n",
    "问题：XXXXXXX\n",
    "答案：XXXXXXX\n",
    "\n",
    "问题：XXXXXXX\n",
    "答案：XXXXXXX\n",
    "\n",
    "内容如下：\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "    c = OpenAI(\n",
    "        api_key=token,\n",
    "        base_url=url,\n",
    "    )\n",
    "    stream = c.chat.completions.create(\n",
    "        model=\"gxllm\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=20000,\n",
    "        stream=False,\n",
    "    )\n",
    "    text = stream.choices[0].message.content\n",
    "    return text\n",
    "\n",
    "\n",
    "def convert_text_to_json(\n",
    "        text: str,\n",
    "        question_pattern: str | None = None,\n",
    "        answer_pattern: str | None = None\n",
    "):\n",
    "    question_pattern = question_pattern or r\"问题[\\d]?[:|：]+\"\n",
    "    answer_pattern = answer_pattern or r\"答案[\\d]?[:|：]+\"\n",
    "    content = []\n",
    "    text = text.replace(\"```\", \"\")\n",
    "    re_ = re.split(question_pattern, text)\n",
    "    for i, r in enumerate(re_):\n",
    "        if not r:\n",
    "            continue\n",
    "        qa = re.split(answer_pattern, r)\n",
    "        if len(qa) < 2:\n",
    "            continue\n",
    "        content.append({\n",
    "            \"index\": shared.snow.sid(),\n",
    "            \"conversations\": [\n",
    "                {\n",
    "                    \"from\": \"user\",\n",
    "                    \"value\": qa[0].strip().replace(\"\\n\", \"\"),\n",
    "                },\n",
    "                {\n",
    "                    \"from\": \"assistant\",\n",
    "                    \"value\": qa[1].strip().replace(\"\\n\", \"\"),\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "    return \n",
    "\n",
    "\n",
    "def extract_question_from_llm(\n",
    "        text: str,\n",
    "        token: str = \"qwen\",\n",
    "        url: str = \"http://10.133.95.100:9100/v1\",\n",
    "        count: int = 50,\n",
    "):\n",
    "    if not text:\n",
    "        raise ValueError(\"context is invalid\")\n",
    "    if count:\n",
    "        count = f\"数量不少于{count}个\"\n",
    "    else:\n",
    "        count = \"提取尽可能多的问题\"\n",
    "    prompt = f\"\"\"\n",
    "根据下述内容提炼问题，要求{count}，不得重复。\n",
    "\n",
    "{text}\n",
    "\n",
    "\"\"\"\n",
    "    c = OpenAI(\n",
    "        api_key=token,\n",
    "        base_url=url,\n",
    "    )\n",
    "    stream = c.chat.completions.create(\n",
    "        model=\"qwen\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=20000,\n",
    "        stream=False,\n",
    "    )\n",
    "    text = stream.choices[0].message.content\n",
    "    return text\n",
    "\n",
    "\n",
    "def extract_question_from_text(\n",
    "        input_dir: str,\n",
    "        output_dir: str,\n",
    "        token: str = \"qwen\",\n",
    "        url: str = \"http://10.133.95.100:9100/v1\",\n",
    "        count: int = 50,\n",
    "):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir(\n",
    "            parents=True, \n",
    "            exist_ok=True,\n",
    "        )\n",
    "    input_dir = Path(input_dir)\n",
    "    for fn in input_dir.glob(\"*.txt\"):\n",
    "        with Path(fn).open(mode=\"r\", encoding=\"utf8\") as f1:\n",
    "            content = f1.read()\n",
    "            res = extract_question_from_llm(\n",
    "                text=content, token=token, url=url, count=count,\n",
    "            )\n",
    "            print(res)\n",
    "            with Path(output_dir, fn.name).open(mode=\"w\", encoding=\"utf8\") as f2:\n",
    "                f2.write(res)\n",
    "\n",
    "\n",
    "def convert_qa_to_json(\n",
    "        input_dir: str,\n",
    "        output_path: str,\n",
    "        question_pattern: str | None = None,\n",
    "        answer_pattern: str | None = None\n",
    "):\n",
    "    content = []\n",
    "    files = os.listdir(input_dir)\n",
    "    for file in files:\n",
    "        file_path = Path(input_dir) / file\n",
    "        if file_path.suffix not in [\".txt\"]:\n",
    "            continue\n",
    "        with file_path.open(mode=\"r\", encoding=\"utf8\") as f:\n",
    "            txt = f.read()\n",
    "            j = convert_text_to_json(\n",
    "                txt,\n",
    "                question_pattern=question_pattern,\n",
    "                answer_pattern=answer_pattern,\n",
    "            )\n",
    "            content = content + j\n",
    "    if content:\n",
    "        with Path(output_path).open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "            j = json.dumps(content, ensure_ascii=False, indent=2)\n",
    "            f.write(j)\n",
    "            \n",
    "\n",
    "def extract_qa_from_txt(\n",
    "        input_path: str | Path,\n",
    "        count: int | None = None,\n",
    "        url: str = \"http://10.133.95.100:9100/v1\",\n",
    "        min_tokens: int = 0,\n",
    "        max_tokens: int = 0\n",
    "):\n",
    "    ret = None\n",
    "    with Path(input_path).open(\"r\", encoding=\"utf8\") as f:\n",
    "        content = f.read()\n",
    "        length = len(content)\n",
    "        if min_tokens and max_tokens and min_tokens < length < max_tokens:\n",
    "            ret = extract_qa_from_llm(\n",
    "                content,\n",
    "                url=url,\n",
    "                count=count\n",
    "            )\n",
    "    return ret\n",
    "\n",
    "\n",
    "def extract_answers_from_text(\n",
    "        input_dir: str | Path,\n",
    "        output_dir: str | Path,\n",
    "        url: str = \"http://10.133.95.100:9100/v1\",\n",
    "        token: str = \"qwen\",\n",
    "        max_tokens: int = 4096\n",
    "):\n",
    "    ret = []\n",
    "    c = OpenAI(\n",
    "        api_key=token,\n",
    "        base_url=url,\n",
    "    )\n",
    "    prompt = \"\"\"\n",
    "根据下述内容回答问题：\n",
    "{context}\n",
    "\n",
    "请问：{question}\n",
    "\"\"\"\n",
    "    for fn in Path(input_dir, \"ques\").glob(\"*.txt\"):\n",
    "        print(\"-\" * 10, fn.name, \"-\" * 10)\n",
    "        with (Path(input_dir, \"ques\", fn.name).open(mode=\"r\", encoding=\"utf8\") as f1, \n",
    "              Path(input_dir, \"txt\", fn.name).open(mode=\"r\", encoding=\"utf8\") as f2):\n",
    "            context = f2.read()\n",
    "            ret = []\n",
    "            for q in f1.readlines():\n",
    "                q = re.sub(\"^(\\d+\\.?\\s?)\", \"\", q).replace(\"\\n\", \" \").strip()\n",
    "                p = prompt.format(context=context, question=q)\n",
    "                stream = c.chat.completions.create(\n",
    "                    model=\"qwen\",\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": p,\n",
    "                        }\n",
    "                    ],\n",
    "                    max_tokens=max_tokens,\n",
    "                    stream=False,\n",
    "                )\n",
    "                ans = stream.choices[0].message.content\n",
    "                print(q)\n",
    "                print(ans)\n",
    "                ret.append({\n",
    "                    \"question\": q,\n",
    "                    \"answer\": ans,\n",
    "                })\n",
    "            if ret:\n",
    "                with Path(output_dir, fn.name).open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "                    j = json.dumps(ret, indent=2, ensure_ascii=False)\n",
    "                    f.write(j)\n",
    "                \n",
    "\n",
    "def extract_qa_from_txt_dir(\n",
    "        input_dir: str, \n",
    "        output_dir: str,\n",
    "        count: int | None = None,\n",
    "        url: str = \"http://10.133.95.100:9100/v1\",\n",
    "        min_tokens: int = 0,\n",
    "        max_tokens: int = 0,\n",
    "):\n",
    "    for i, file in enumerate(os.listdir(input_dir)):\n",
    "        print(\"processing file: {}\".format(file))\n",
    "        ret = extract_qa_from_txt(\n",
    "            input_path=Path(input_dir, file),\n",
    "            count=count,\n",
    "            url=url,\n",
    "            min_tokens=min_tokens,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        # with Path(output_dir, \"{:0>10}.txt\".format(i)).open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "        #     f.write(ret)\n",
    "        if ret:\n",
    "            print(ret)\n",
    "            with Path(output_dir, file).open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "                f.write(ret)\n",
    "                \n",
    "\n",
    "def convert_qa_json_from_root_dir(\n",
    "        root_dir: str,\n",
    "        output_path: str,\n",
    "        sub_dirs: List[str] | None = None,\n",
    "        question_pattern: str | None = None,\n",
    "        answer_pattern: str | None = None\n",
    "):\n",
    "    content = []\n",
    "    sub_dirs = sub_dirs or [\"qa\", ]\n",
    "    for d in os.listdir(root):\n",
    "        p = Path(root_dir, d, *sub_dirs)\n",
    "        for f in os.listdir(p.absolute()):\n",
    "            with Path(p, f).open(mode=\"r\", encoding=\"utf8\") as f:\n",
    "                c = f.read()\n",
    "                qa = convert_text_to_json(\n",
    "                    c,\n",
    "                    question_pattern=question_pattern,\n",
    "                    answer_pattern=answer_pattern,\n",
    "                )\n",
    "                content += qa\n",
    "    if content:\n",
    "        print(output_path)\n",
    "        with Path(output_path).open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "            j = json.dumps(content, ensure_ascii=False, indent=2)\n",
    "            f.write(j)\n",
    "            \n",
    "            \n",
    "def merge_txt(\n",
    "        root_dir: str | Path,\n",
    "        output_path: str | Path,\n",
    "):\n",
    "    output = []\n",
    "    root_dir = Path(root_dir)\n",
    "    output_path = Path(output_path)\n",
    "    for fn in root_dir.iterdir():\n",
    "        p = root_dir / fn.name / \"doc\"\n",
    "        for fn in p.iterdir():\n",
    "            with Path(fn).open(mode=\"r\", encoding=\"utf8\") as f:\n",
    "                c = f.read()\n",
    "                j = json.loads(c)\n",
    "                output.extend(j)\n",
    "    if output:\n",
    "        with Path(output_path).open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "            f.write(json.dumps(output, ensure_ascii=False, indent=2))\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "data_dir = Path(\"D:/Desktop/1\")\n",
    "output = []\n",
    "for fn in data_dir.iterdir():\n",
    "    with fn.open(mode=\"r\", encoding=\"utf8\") as f:\n",
    "        c = f.read()\n",
    "        j = json.loads(c)\n",
    "        for i in j:\n",
    "            # output.append({\n",
    "            #     \"type\": \"chatml\",\n",
    "            #     \"messages\": [\n",
    "            #         {\n",
    "            #             \"role\": \"system\",\n",
    "            #             \"content\": \"You are a helpful assistant.\"\n",
    "            #         },\n",
    "            #         {\n",
    "            #             \"role\": \"user\",\n",
    "            #             \"content\": i[\"question\"],\n",
    "            #         },\n",
    "            #         {\n",
    "            #             \"role\": \"assistant\",\n",
    "            #             \"content\": i[\"answer\"],\n",
    "            #         }                \n",
    "            #     ],\n",
    "            #     \"source\": \"unknown\"\n",
    "            # })\n",
    "            output.append({\n",
    "                \"input\": \"\",\n",
    "                \"question\": i[\"question\"],\n",
    "                \"answer\": i[\"answer\"],\n",
    "            })\n",
    "if output:\n",
    "    with Path(\"D:/Desktop/dataset.json\").open(mode=\"w+\", encoding=\"utf8\") as f:\n",
    "        f.write(json.dumps(output, ensure_ascii=False, indent=2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T08:11:14.426326Z",
     "start_time": "2024-02-29T08:11:14.243786Z"
    }
   },
   "id": "4815d604479b60b8",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xb0 in position 14: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 23\u001B[0m\n\u001B[0;32m      2\u001B[0m name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1001-个人所得税\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# convert_doc_to_text(\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;66;03m#     input_dir=\"D:/Desktop/dataset/000000/\" + name + \"/doc\",\u001B[39;00m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m#     output_dir=\"D:/Desktop/dataset/000000/\" + name + \"/txt\",\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m#     output_dir=Path(\"D:/Desktop/dataset/000000/\", name, \"qa\"),\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m---> 23\u001B[0m \u001B[43mmerge_txt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mroot_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mroot\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mD:/Desktop/data.json\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[0;32m     26\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# extract_qa_from_txt_dir(\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m#     input_dir=str(Path(root, name, \"txt\").absolute()),\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m#     output_dir=str(Path(root, name, \"qa\").absolute()),\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     44\u001B[0m \u001B[38;5;66;03m#     output_path=\"D:/Desktop/json.json\"\u001B[39;00m\n\u001B[0;32m     45\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[1], line 361\u001B[0m, in \u001B[0;36mmerge_txt\u001B[1;34m(root_dir, output_path)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m p\u001B[38;5;241m.\u001B[39miterdir():\n\u001B[0;32m    360\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m Path(fn)\u001B[38;5;241m.\u001B[39mopen(mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m, encoding\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf8\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m--> 361\u001B[0m         c \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    362\u001B[0m         j \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(c)\n\u001B[0;32m    363\u001B[0m         output\u001B[38;5;241m.\u001B[39mextend(j)\n",
      "File \u001B[1;32m<frozen codecs>:322\u001B[0m, in \u001B[0;36mdecode\u001B[1;34m(self, input, final)\u001B[0m\n",
      "\u001B[1;31mUnicodeDecodeError\u001B[0m: 'utf-8' codec can't decode byte 0xb0 in position 14: invalid start byte"
     ]
    }
   ],
   "source": [
    "root = \"D:/Desktop/dataset/000000\"\n",
    "name = \"1001-个人所得税\"\n",
    "\n",
    "# convert_doc_to_text(\n",
    "#     input_dir=\"D:/Desktop/dataset/000000/\" + name + \"/doc\",\n",
    "#     output_dir=\"D:/Desktop/dataset/000000/\" + name + \"/txt\",\n",
    "#     split_size=10000,\n",
    "#     # split_pattern=r\"第[一二三四五六七八九十百零]+章\"\n",
    "#     # split_pattern=r\"第[一二三四五六七八九十百零]+条\"\n",
    "#     # split_pattern=r\"\\d+[\\u4e00-\\u9fa5]+(\\r?\\n)+\", \n",
    "# )\n",
    "# \n",
    "# extract_question_from_text(\n",
    "#         input_dir=\"D:/Desktop/dataset/000000/\" + name + \"/txt\",\n",
    "#         output_dir=\"D:/Desktop/dataset/000000/\" + name + \"/ques\",\n",
    "# )\n",
    "\n",
    "# extract_answers_from_text(\n",
    "#     input_dir=Path(\"D:/Desktop/dataset/000000\",  name),\n",
    "#     output_dir=Path(\"D:/Desktop/dataset/000000/\", name, \"qa\"),\n",
    "# )\n",
    "\n",
    "merge_txt(\n",
    "    root_dir=root,\n",
    "    output_path=\"D:/Desktop/data.json\"\n",
    ")\n",
    "\n",
    "# extract_qa_from_txt_dir(\n",
    "#     input_dir=str(Path(root, name, \"txt\").absolute()),\n",
    "#     output_dir=str(Path(root, name, \"qa\").absolute()),\n",
    "#     # count=30,\n",
    "#     url=\"http://10.133.95.100:9100/v1\",\n",
    "#     min_tokens=20,\n",
    "#     max_tokens=20000,\n",
    "# )\n",
    "# \n",
    "# convert_qa_to_json(\n",
    "#     input_dir=\"D:/Desktop/3\",\n",
    "#     output_path=\"D:/Desktop/json.json\",\n",
    "# )\n",
    "\n",
    "# convert_qa_json_from_root_dir(\n",
    "#     root_dir=root,\n",
    "#     output_path=\"D:/Desktop/json.json\"\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T08:09:59.990075Z",
     "start_time": "2024-02-29T08:09:59.957778Z"
    }
   },
   "id": "d506b433cb3a574c",
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
